# ====================== 词嵌入与文本预处理模板 ======================
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from collections import Counter
import numpy as np

# 1. 构建词汇表
class Vocabulary:
    def __init__(self, min_freq=1):
        self.word2idx = {"<PAD>":0, "<UNK>":1}
        self.idx2word = {0:"<PAD>", 1:"<UNK>"}
        self.min_freq = min_freq
        self.counter = Counter()
    
    def build_vocab(self, texts):
        for text in texts:
            self.counter.update(text.split())
        
        idx = 2
        for word, count in self.counter.items():
            if count >= self.min_freq:
                self.word2idx[word] = idx
                self.idx2word[idx] = word
                idx += 1

# 2. 自定义数据集类
class TextDataset(Dataset):
    def __init__(self, texts, labels, vocab, max_len=50):
        self.texts = [self._numericalize(text, vocab) for text in texts]
        self.labels = labels
        self.max_len = max_len
    
    def _numericalize(self, text, vocab):
        return [vocab.word2idx.get(word, 1) for word in text.split()]
    
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        text = self.texts[idx][:self.max_len]
        pad_len = self.max_len - len(text)
        return {
            "input_ids": torch.tensor(text + [0]*pad_len, dtype=torch.long),
            "label": torch.tensor(self.labels[idx], dtype=torch.float)
        }

# 3. 实现Word2Vec（Skip-Gram）
class SkipGram(nn.Module):
    def __init__(self, vocab_size, embed_dim):
        super().__init__()
        self.embeddings = nn.Embedding(vocab_size, embed_dim)
        self.linear = nn.Linear(embed_dim, vocab_size)
    
    def forward(self, inputs):
        embeds = self.embeddings(inputs)  # (batch, embed_dim)
        output = self.linear(embeds)      # (batch, vocab_size)
        return output

# ====================== Transformer基础模板 ======================
class MultiHeadAttention(nn.Module):
    def __init__(self, d_model=512, n_heads=8):
        super().__init__()
        self.d_model = d_model
        self.n_heads = n_heads
        self.d_head = d_model // n_heads
        
        self.Wq = nn.Linear(d_model, d_model)
        self.Wk = nn.Linear(d_model, d_model)
        self.Wv = nn.Linear(d_model, d_model)
        self.out = nn.Linear(d_model, d_model)
    
    def forward(self, Q, K, V, mask=None):
        batch_size = Q.size(0)
        
        # 线性变换并分头
        Q = self.Wq(Q).view(batch_size, -1, self.n_heads, self.d_head).transpose(1,2)
        K = self.Wk(K).view(batch_size, -1, self.n_heads, self.d_head).transpose(1,2)
        V = self.Wv(V).view(batch_size, -1, self.n_heads, self.d_head).transpose(1,2)
        
        # 计算注意力分数
        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.d_head)
        if mask is not None:
            scores = scores.masked_fill(mask==0, -1e9)
        attn = torch.softmax(scores, dim=-1)
        
        # 上下文向量
        context = torch.matmul(attn, V)
        context = context.transpose(1,2).contiguous().view(batch_size, -1, self.d_model)
        return self.out(context)

class TransformerEncoderLayer(nn.Module):
    def __init__(self, d_model, n_heads, ff_dim, dropout=0.1):
        super().__init__()
        self.self_attn = MultiHeadAttention(d_model, n_heads)
        self.norm1 = nn.LayerNorm(d_model)
        self.ffn = nn.Sequential(
            nn.Linear(d_model, ff_dim),
            nn.ReLU(),
            nn.Linear(ff_dim, d_model)
        )
        self.norm2 = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x):
        attn_out = self.self_attn(x, x, x)
        x = self.norm1(x + self.dropout(attn_out))
        ffn_out = self.ffn(x)
        x = self.norm2(x + self.dropout(ffn_out))
        return x

# ====================== BERT微调模板 ======================
from transformers import BertTokenizer, BertForSequenceClassification

# 1. 数据准备
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

class BertDataset(Dataset):
    def __init__(self, texts, labels, max_len=128):
        self.texts = texts
        self.labels = labels
        self.max_len = max_len
    
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        encoding = tokenizer(
            self.texts[idx],
            max_length=self.max_len,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'label': torch.tensor(self.labels[idx], dtype=torch.long)
        }

# 2. 加载预训练模型
model = BertForSequenceClassification.from_pretrained(
    'bert-base-uncased',
    num_labels=2  # 根据任务调整
)

# 3. 训练配置
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = model.to(device)
optimizer = optim.AdamW(model.parameters(), lr=2e-5)
criterion = nn.CrossEntropyLoss()

# 4. 训练循环
def train_epoch(model, dataloader):
    model.train()
    total_loss = 0
    for batch in dataloader:
        optimizer.zero_grad()
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['label'].to(device)
        
        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    return total_loss / len(dataloader)

# ====================== GPT文本生成模板 ======================
from transformers import GPT2Tokenizer, GPT2LMHeadModel

# 1. 初始化模型和分词器
tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')
model = GPT2LMHeadModel.from_pretrained('gpt2-medium')
tokenizer.pad_token = tokenizer.eos_token  # 设置填充token

# 2. 文本生成函数
def generate_text(prompt, max_length=100, temperature=0.7):
    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)
    
    output = model.generate(
        input_ids,
        max_length=max_length,
        temperature=temperature,
        do_sample=True,
        top_k=50,
        pad_token_id=tokenizer.eos_token_id
    )
    return tokenizer.decode(output[0], skip_special_tokens=True)

# 3. 自定义训练（继续预训练）
train_dataset = ...  # 需准备文本数据集

training_args = transformers.TrainingArguments(
    output_dir='./results',
    num_train_epochs=3,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=8,
    learning_rate=5e-5,
    fp16=True  # 启用混合精度训练
)

trainer = transformers.Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset
)

trainer.train()

# ====================== 实用工具函数 ======================
# 1. 学习率预热
def warmup(current_step, warmup_steps=1000):
    return min(current_step ** (-0.5), current_step * warmup_steps ** (-1.5))

# 2. 梯度裁剪
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

# 3. 模型保存与加载
def save_model(model, path):
    torch.save({
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict()
    }, path)

def load_model(path):
    checkpoint = torch.load(path)
    model.load_state_dict(checkpoint['model_state_dict'])
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
代码说明与使用技巧：

词嵌入模块：

包含完整的词汇表构建流程

实现Skip-Gram模型架构

支持自定义文本数据集类

可直接扩展到GloVe等其他嵌入方法

Transformer模块：

实现多头注意力机制

构建标准Transformer编码器层

包含LayerNorm和残差连接

可堆叠构建深层Transformer

BERT微调模块：

集成Hugging Face Transformers库

支持序列分类、问答等多种任务

包含注意力掩码处理

适配GPU混合精度训练

GPT生成模块：

实现文本生成功能

支持温度调节和Top-K采样

包含继续预训练流程

可扩展到对话生成等场景