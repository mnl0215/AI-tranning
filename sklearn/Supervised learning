1. 线性回归 (Linear Regression)
python
# 导入库
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score

# 加载数据 (示例路径，需替换实际路径)
data = pd.read_csv("your_data.csv")
X = data.drop("target", axis=1)  # 特征矩阵
y = data["target"]               # 连续型目标变量

# 数据预处理
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 分割数据集
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, 
    test_size=0.2, 
    random_state=42
)

# 初始化模型
model = LinearRegression(fit_intercept=True)

# 训练模型
model.fit(X_train, y_train)

# 预测与评估
y_pred = model.predict(X_test)
print(f"系数: {model.coef_}")
print(f"截距: {model.intercept_:.2f}")
print(f"MSE: {mean_squared_error(y_test, y_pred):.2f}")
print(f"R²: {r2_score(y_test, y_pred):.2f}")

# 交叉验证
cv_scores = cross_val_score(model, X_scaled, y, cv=5, scoring='r2')
print(f"交叉验证 R²: {cv_scores.mean():.2f} (±{cv_scores.std():.2f})")

# 可视化残差
plt.figure(figsize=(10,6))
plt.scatter(y_pred, y_pred - y_test, c='blue', alpha=0.5)
plt.hlines(y=0, xmin=y_pred.min(), xmax=y_pred.max(), colors='red')
plt.title("残差分布图")
plt.xlabel("预测值")
plt.ylabel("残差")
plt.show()
2. 逻辑回归 (Logistic Regression)
python
# 导入库
import pandas as pd
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (classification_report, 
                             confusion_matrix, 
                             roc_auc_score,
                             RocCurveDisplay)

# 加载数据
data = pd.read_csv("your_data.csv")
X = data.drop("target", axis=1)  # 特征矩阵
y = data["target"]               # 二分类目标变量 (0/1)

# 数据标准化
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 分割数据集
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y,
    test_size=0.2,
    random_state=42,
    stratify=y
)

# 初始化模型
model = LogisticRegression(
    penalty='l2',
    C=1.0,
    solver='lbfgs',
    max_iter=1000,
    class_weight='balanced'
)

# 超参数调优
param_grid = {
    'C': [0.001, 0.01, 0.1, 1, 10],
    'penalty': ['l1', 'l2']
}

grid_search = GridSearchCV(
    model,
    param_grid,
    cv=5,
    scoring='roc_auc',
    n_jobs=-1
)
grid_search.fit(X_train, y_train)

# 最佳模型
best_model = grid_search.best_estimator_
print(f"最佳参数: {grid_search.best_params_}")

# 评估模型
y_pred = best_model.predict(X_test)
y_proba = best_model.predict_proba(X_test)[:, 1]

print("\n分类报告:")
print(classification_report(y_test, y_pred))

print(f"AUC-ROC: {roc_auc_score(y_test, y_proba):.2f}")

# 可视化混淆矩阵
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title("混淆矩阵")
plt.xlabel("预测标签")
plt.ylabel("真实标签")
plt.show()

# ROC曲线
RocCurveDisplay.from_estimator(best_model, X_test, y_test)
plt.title("ROC曲线")
plt.show()
3. K近邻 (KNN)
python
# 导入库
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import classification_report

# 加载数据
data = pd.read_csv("your_data.csv")
X = data.drop("target", axis=1)
y = data["target"]  # 分类目标变量

# 归一化处理
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)

# 分割数据集
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y,
    test_size=0.2,
    random_state=42,
    stratify=y
)

# 选择最佳K值
k_values = range(1, 21)
cv_scores = []

for k in k_values:
    knn = KNeighborsClassifier(n_neighbors=k)
    scores = cross_val_score(knn, X_train, y_train, cv=5, scoring='accuracy')
    cv_scores.append(scores.mean())

# 可视化K值选择
plt.figure(figsize=(10,6))
plt.plot(k_values, cv_scores, marker='o')
plt.title("K值选择曲线")
plt.xlabel("K值")
plt.ylabel("交叉验证准确率")
plt.xticks(k_values)
plt.grid(True)
plt.show()

# 最佳K值模型
best_k = np.argmax(cv_scores) + 1  # 索引从0开始
knn = KNeighborsClassifier(n_neighbors=best_k)
knn.fit(X_train, y_train)

# 评估
y_pred = knn.predict(X_test)
print(f"最佳K值: {best_k}")
print(classification_report(y_test, y_pred))

# 特征重要性分析 (KNN无内置方法，使用排列重要性)
from sklearn.inspection import permutation_importance

result = permutation_importance(
    knn, X_test, y_test,
    n_repeats=10,
    random_state=42
)

importance_df = pd.DataFrame({
    'feature': X.columns,
    'importance': result.importances_mean
}).sort_values('importance', ascending=False)

print("\n特征重要性:")
print(importance_df)
4. 决策树 (Decision Tree)
python
# 导入库
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.tree import (DecisionTreeClassifier, 
                          plot_tree)
from sklearn.metrics import classification_report
import matplotlib.pyplot as plt

# 加载数据
data = pd.read_csv("your_data.csv")
X = data.drop("target", axis=1)
y = data["target"]  # 分类目标变量

# 分割数据集
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    random_state=42,
    stratify=y
)

# 初始化模型
model = DecisionTreeClassifier(
    max_depth=5,
    min_samples_split=10,
    criterion='gini',
    random_state=42
)

# 训练模型
model.fit(X_train, y_train)

# 可视化决策树
plt.figure(figsize=(25,15))
plot_tree(
    model,
    feature_names=X.columns,
    class_names=y.unique().astype(str),
    filled=True,
    rounded=True,
    proportion=True
)
plt.title("决策树可视化")
plt.show()

# 评估模型
y_pred = model.predict(X_test)
print(classification_report(y_test, y_pred))

# 特征重要性分析
importance = pd.Series(
    model.feature_importances_,
    index=X.columns
).sort_values(ascending=False)

print("\n特征重要性:")
print(importance)

# 保存决策规则
from sklearn.tree import export_text

tree_rules = export_text(
    model,
    feature_names=list(X.columns)
)
with open("decision_rules.txt", "w") as f:
    f.write(tree_rules)
5. 随机森林 (Random Forest)
python
# 导入库
import pandas as pd
import numpy as np
import seaborn as sns
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import (classification_report,
                             confusion_matrix,
                             accuracy_score)

# 加载数据
data = pd.read_csv("your_data.csv")
X = data.drop("target", axis=1)
y = data["target"]  # 分类目标变量

# 分割数据集
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    random_state=42,
    stratify=y
)

# 初始化模型
model = RandomForestClassifier(
    n_estimators=200,
    max_depth=10,
    min_samples_split=5,
    max_features='sqrt',
    class_weight='balanced',
    random_state=42,
    n_jobs=-1
)

# 超参数优化
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [5, 10, None],
    'min_samples_split': [2, 5, 10]
}

grid_search = GridSearchCV(
    model,
    param_grid,
    cv=5,
    scoring='f1_macro',
    verbose=1,
    n_jobs=-1
)
grid_search.fit(X_train, y_train)

# 最佳模型
best_model = grid_search.best_estimator_
print(f"最佳参数: {grid_search.best_params_}")

# 评估模型
y_pred = best_model.predict(X_test)
print("\n分类报告:")
print(classification_report(y_test, y_pred))

# 特征重要性可视化
importance = pd.DataFrame({
    'Feature': X.columns,
    'Importance': best_model.feature_importances_
}).sort_values('Importance', ascending=False)

plt.figure(figsize=(12,8))
sns.barplot(x='Importance', y='Feature', data=importance.head(15))
plt.title("Top 15 重要特征")
plt.show()

# 保存模型
import joblib
joblib.dump(best_model, 'best_random_forest.pkl')

# 加载模型示例
# loaded_model = joblib.load('best_random_forest.pkl')
模型对比总结表
模型	优势	劣势	适用场景
线性回归	解释性强，计算速度快	只能捕捉线性关系	连续数值预测（房价、销量）
逻辑回归	概率输出，支持正则化	线性决策边界	二分类/多分类问题
K近邻	无需训练，简单易用	计算效率低，高维效果差	小数据集，局部模式识别
决策树	可视化强，处理非线性	容易过拟合	可解释性优先场景
随机森林	准确率高，抗过拟合能力强	计算资源消耗大	通用分类/回归问题
